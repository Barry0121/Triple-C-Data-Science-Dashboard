{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471e0bb2-44d7-4bcf-ac82-e2598020f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0c2e53-a382-4f8c-aa73-0081b4d01e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only for the first time to create the csv file\n",
    "# scrapped_links = open(\"scrapped.csv\", \"a+\")\n",
    "# fieldnames = ['link']\n",
    "# writer = csv.DictWriter(scrapped_links, fieldnames=fieldnames)\n",
    "# writer.writeheader()\n",
    "# scrapped_links.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb758f6-f00a-421d-ab38-254c9867c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = [\n",
    "    'url',\n",
    "    'post_id',\n",
    "    'post_date',\n",
    "    'description',\n",
    "    'num_beds',\n",
    "    'num_baths',\n",
    "    'price',\n",
    "    'address',\n",
    "    'sqft',\n",
    "    'features',\n",
    "    'scrape_date'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d582714b-0451-4437-8068-74cc58e205d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First time only: to set up the csv file\n",
    "# with open('craigslist.csv', mode='a+') as df:\n",
    "#     writer = csv.DictWriter(df, fieldnames = fieldnames)\n",
    "#     writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69252f04-3bdb-4abd-9f9d-7939779f4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_housing_links(dist = 10, postal = 92037,\n",
    "    url = \"https://sandiego.craigslist.org/d/apartments-housing-for-rent/search/apa?s={page}&availabilityMode=0&postal={zip_code}&search_distance={miles}\" \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Function that takes in two parameters, distance in miles and the zip code we are performing our search in.\n",
    "    It will search postings within the set radius around the zip code and save its links\n",
    "    dist: int\n",
    "    postal: int\n",
    "    url: string of the website of the craigslist site, default is the san diego craigslist.\n",
    "    returns: a set of links to scrape\n",
    "    \"\"\"\n",
    "    # Find the county we are searching in \n",
    "    county = re.match(\"^https:\\/\\/([\\w]+).craigslist.org\", url).group(1)\n",
    "    posting_links = set()\n",
    "    # Find all of the links that redirects to the posting on craigslist\n",
    "    temp_url = url.format(miles = dist, zip_code = postal, page = 0)\n",
    "    listing = requests.get(temp_url)\n",
    "    content = listing.text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    # find the number of search results\n",
    "    total_count = int(soup.find('span', attrs = {'class': 'totalcount'}).text)\n",
    "    # We land on the first page and every page lists 120 postings\n",
    "    current, num_results_on_a_page = 0, 120\n",
    "    while current < total_count:\n",
    "        # Access the website and parse the webpage\n",
    "        time.sleep(random.randint(2, 10))\n",
    "        temp_url = url.format(miles = dist, zip_code = postal, page = current)\n",
    "        listing = requests.get(temp_url)\n",
    "        content = listing.text\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        htmls = soup.find_all('a', attrs = {'class': 'result-title hdrlnk'})\n",
    "        for link in htmls:\n",
    "            # Filter out the sponsored results\n",
    "            address = link.get('href')\n",
    "            if county in address:\n",
    "                posting_links.add(address)\n",
    "        current += num_results_on_a_page\n",
    "    return posting_links      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39a40ef-92c7-41c3-9e20-5de73ba5b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_basic_info(post):\n",
    "    \"\"\"\n",
    "    Input: post takes in a soup object of a craigslist posting\n",
    "    and returns the post_id and post_date in a tuple\n",
    "    \"\"\"    \n",
    "    url = post.find(\"meta\", property=\"og:url\").get('content')\n",
    "    if url is None:\n",
    "        post_id = None\n",
    "    else:\n",
    "        post_id = re.search('([\\d]+).html', url).group(1)\n",
    "    post_date = post.find('time', attrs = {'class': 'date timeago'}).get('datetime')\n",
    "    return {\n",
    "        'url': url,\n",
    "        'post_id': post_id,\n",
    "        'post_date': post_date\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a0608b-92e4-4a2f-abf0-3859857260e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listing_info(post):\n",
    "    \"\"\"\n",
    "    Input: post takes in a soup object of a craigslist posting\n",
    "    Returns: a number of bedrooms, bathrooms, price, address, and the size of the listing as a dictionary\n",
    "    \"\"\"\n",
    "    price = post.find('span', attrs = {'class': 'price'})\n",
    "    if price is not None:\n",
    "        price = price.text.strip('$').replace(',', '')\n",
    "    else:\n",
    "        price = -1\n",
    "    temp = post.find('span', attrs = {'class': 'shared-line-bubble'}).text.split('/')\n",
    "    if temp is None:\n",
    "        num_beds = -1\n",
    "        num_baths = -1\n",
    "    else:\n",
    "        temp = [i.strip() for i in temp]\n",
    "        if len(temp) == 2: # listed both the number of bathrooms and bedrooms\n",
    "            num_beds = int(temp[0].lower().strip('br'))\n",
    "            num_baths = temp[1].lower().strip('ba')\n",
    "        elif len(temp) == 1: # Listed one but not the other\n",
    "            if \"br\" in temp[0].lower():\n",
    "                num_beds = temp[0].lower().strip('br')\n",
    "                num_baths = -1\n",
    "            elif \"ba\" in temp[0].lower():\n",
    "                num_beds = -1\n",
    "                num_baths = temp[0].lower.strip(\"ba\")\n",
    "    address = post.find('div', attrs = {'class': 'mapaddress'}) #unable to scrape address if there is none\n",
    "    if address is not None:\n",
    "        address = address.text\n",
    "    sqft = post.find_all('span', attrs = {'class': 'shared-line-bubble'})\n",
    "    if len(sqft) == 2 and 'ft2' in sqft[1].text:\n",
    "        sqft = sqft[1].text.strip('ft2')\n",
    "    else:\n",
    "        sqft = -1\n",
    "    return {\n",
    "        'num_beds': num_beds,\n",
    "        'num_baths': num_baths,\n",
    "        'price': price,\n",
    "        'address': address,\n",
    "        'sqft': sqft\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd6c242-cfb9-4129-9e97-f8518cc0d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_desc(post):\n",
    "    \"\"\"\n",
    "    Input: post takes in a soup object of a craigslist posting\n",
    "    Returns: a dictionary of the string of the description for the posting by the poster\n",
    "    \"\"\"\n",
    "    description = post.find('section', attrs = {'id': 'postingbody'})\n",
    "    if description is not None:\n",
    "        description = description.text.strip().strip('QR Code Link to This Post\\n\\n\\n')\n",
    "    return {'description': description}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6182570e-583f-40ee-9444-f7e8d2ce57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_features(post, feature_vector = {}):\n",
    "    \"\"\"\n",
    "    Input: post takes in a soup object of a craigslist posting\n",
    "    feature vector is a dictionary of keys with value 0\n",
    "    Returns a number of features indicated by the poster\n",
    "    \"\"\"\n",
    "    # might be modifying every feature vector instead\n",
    "    search = post.find_all('p', attrs = {'class': 'attrgroup'})\n",
    "    \n",
    "    if len(search) == 2: # posting has listed attributes\n",
    "        # app_fee_found = False\n",
    "        attributes = [i.text for i in search[1].find_all('span')]\n",
    "        feature_vector['features'] = attributes\n",
    "        # for attr in attributes:\n",
    "            # if \"application fee\" in attr:\n",
    "                # app_fee_found = True\n",
    "                # fee = re.search(\"\\$([\\d]+)\", attr)\n",
    "                # if fee is not None:\n",
    "                    # feature_vector['application fee'] = fee.group(1)\n",
    "                # else:\n",
    "                    # feature_vector['application fee'] = -1\n",
    "            # else:\n",
    "                # feature_vector[attr] = 1\n",
    "        # if app_fee_found == False:\n",
    "            # feature_vector['application fee'] = -1\n",
    "    else:\n",
    "        feature_vector['features'] = []\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "333c1cf9-efd3-4285-8e4a-96ebf79b8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_post(post):\n",
    "    if post.find('div', attrs = {'class': 'removed'}) is not None: # if the post is removed\n",
    "        return None\n",
    "    row = dict()\n",
    "    post_info = scrape_basic_info(post)\n",
    "    row.update(post_info)\n",
    "    description = scrape_desc(post)\n",
    "    row.update(description)\n",
    "    info = scrape_listing_info(post)\n",
    "    row.update(info)\n",
    "    features = scrape_features(post)\n",
    "    row.update(features)\n",
    "    row['scrape_date'] = datetime.today()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2252836-81a1-41a4-8d1c-254a3e5ef623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(post):\n",
    "    \"\"\"\n",
    "    Writes to the csv file\n",
    "    \"\"\"\n",
    "    with open('craigslist.csv', mode='a+') as df:\n",
    "        writer = csv.DictWriter(df, fieldnames = fieldnames, extrasaction='ignore', restval = None)\n",
    "        parsed_data = scrape_post(post)\n",
    "        if parsed_data is not None:\n",
    "            writer.writerow(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cacc772-b9b9-41cb-a568-aa921b6b4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_webpages(links):\n",
    "    \"\"\"\n",
    "    links is an iterable\n",
    "    returns a list of soup objects\n",
    "    \"\"\"\n",
    "    for link in links:\n",
    "        time.sleep(random.randint(2, 10))\n",
    "        listing = requests.get(link)\n",
    "        content = listing.text\n",
    "        soup = BeautifulSoup(content)\n",
    "        if soup is None:\n",
    "            continue\n",
    "        write_to_csv(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3519b412-c425-4482-9677-9b1dfefbf5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pages():\n",
    "    \"\"\"\n",
    "    Pipeline to streamline scraping for websites and download the webpage\n",
    "    \"\"\"\n",
    "    # Download links of relevent postings\n",
    "    links = scrape_housing_links()\n",
    "    \n",
    "    # Add new postings to the list of scrapped postings\n",
    "    with open('scrapped.csv', mode='r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None)  # skip the headers\n",
    "        scrapped_links = set([link[0] for link in reader])\n",
    "    \n",
    "    with open('scrapped.csv', mode='a') as f:\n",
    "        # remove duplicate links that has been downloaded before\n",
    "        # Still need to remove duplicates with the same post id but different url (due to change in title of the post)\n",
    "        writer = csv.DictWriter(f, fieldnames = [\"link\"])\n",
    "        to_download = links.difference(scrapped_links) \n",
    "        for link in to_download:\n",
    "            temp = {\"link\": link}\n",
    "            writer.writerow(temp)\n",
    "        \n",
    "    # Scrape the webpage and save it to a csv file\n",
    "    scrape_webpages(to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d94d1030-ff01-4ff4-9102-bacb20960eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescrape():\n",
    "    \"\"\"\n",
    "    Scrapped the pages that were left off due to an interruption of the scrape_pages\n",
    "    function that left a discrepency between the number of scrapped pages and actual\n",
    "    entries in craigslist.csv\n",
    "    \"\"\"\n",
    "    with open('scrapped.csv', mode='r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None)  # skip the headers\n",
    "        links = set([link[0] for link in reader])\n",
    "    df = pd.read_csv('craigslist.csv')\n",
    "    scrapped_listings = set(df.loc[:, 'url'].tolist())\n",
    "    to_download = links.difference(scrapped_listings) \n",
    "    scrape_webpages(to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80c1357-e8c9-4bcb-8473-b1f8651aa0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper():\n",
    "    try:\n",
    "        scrape_pages()\n",
    "    except:\n",
    "        rescrape()\n",
    "    finally:\n",
    "        rescrape()\n",
    "        with open('scrape_log.txt', mode = 'a') as f:\n",
    "            f.write('Scrape completed on {}\\n'.format(date.today()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22822d-9733-47c9-ae09-f989f9e2a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63cd64a7-2baa-4ec0-a24c-536bf3549e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74606e0b-747e-4dee-ae9a-cb7e0708df5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddf355-6198-42ac-9a6f-6bd898ac6ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
